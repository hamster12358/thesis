# -*- coding: utf-8 -*-
"""3DCNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IbYMa9RxdvkKTZ6ZLHsKvi7Jqp5Vot27
"""

from tensorflow import keras
from imutils import paths

import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision.models as models
import torchvision.transforms as transforms
from torch.utils import data
from PIL import Image
import pandas as pd
import numpy as np
import imageio
import cv2
import os
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.metrics import balanced_accuracy_score
from sklearn.metrics import f1_score
from sklearn.preprocessing import OneHotEncoder, LabelEncoder
import pickle


def conv3D_output_size(img_size, padding, kernel_size, stride):
    # 3DCNN输出结果维度
    outshape = (np.floor((img_size[0] + 2 * padding[0] - (kernel_size[0] - 1) - 1) / stride[0] + 1).astype(int),
                np.floor((img_size[1] + 2 * padding[1] - (kernel_size[1] - 1) - 1) / stride[1] + 1).astype(int),
                np.floor((img_size[2] + 2 * padding[2] - (kernel_size[2] - 1) - 1) / stride[2] + 1).astype(int))
    return outshape


class CNN3D(nn.Module):
    def __init__(self, t_dim=120, img_x=90, img_y=120, drop_p=0.2, fc_hidden1=256, fc_hidden2=128, num_classes=5):
        super(CNN3D, self).__init__()

        # 设置视频维度
        self.t_dim = t_dim
        self.img_x = img_x
        self.img_y = img_y
        # 全连接层
        self.fc_hidden1, self.fc_hidden2 = fc_hidden1, fc_hidden2
        self.drop_p = drop_p
        self.num_classes = num_classes
        self.ch1, self.ch2 = 32, 48
        self.k1, self.k2 = (5, 5, 5), (3, 3, 3)  # 3d kernel size
        self.s1, self.s2 = (2, 2, 2), (2, 2, 2)  # 3d strides
        self.pd1, self.pd2 = (0, 0, 0), (0, 0, 0)  # 3d padding

        self.conv1_outshape = conv3D_output_size((self.t_dim, self.img_x, self.img_y), self.pd1, self.k1, self.s1)
        self.conv2_outshape = conv3D_output_size(self.conv1_outshape, self.pd2, self.k2, self.s2)

        self.conv1 = nn.Conv3d(in_channels=1, out_channels=self.ch1, kernel_size=self.k1, stride=self.s1,
                               padding=self.pd1)
        self.bn1 = nn.BatchNorm3d(self.ch1)
        self.conv2 = nn.Conv3d(in_channels=self.ch1, out_channels=self.ch2, kernel_size=self.k2, stride=self.s2,
                               padding=self.pd2)
        self.bn2 = nn.BatchNorm3d(self.ch2)
        self.relu = nn.ReLU(inplace=True)
        self.drop = nn.Dropout3d(self.drop_p)
        self.pool = nn.MaxPool3d(2)
        self.fc1 = nn.Linear(self.ch2 * self.conv2_outshape[0] * self.conv2_outshape[1] * self.conv2_outshape[2],
                             self.fc_hidden1)
        self.fc2 = nn.Linear(self.fc_hidden1, self.fc_hidden2)
        self.fc3 = nn.Linear(self.fc_hidden2, self.num_classes)  # 输出层

    def forward(self, x_3d):
        # Conv 1
        x = self.conv1(x_3d)
        x = self.bn1(x)
        x = self.relu(x)
        x = self.drop(x)
        # Conv 2
        x = self.conv2(x)
        x = self.bn2(x)
        x = self.relu(x)
        x = self.drop(x)
        # FC 1 and 2
        x = x.view(x.size(0), -1)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        self.x_tsne = F.dropout(x, p=self.drop_p, training=self.training)
        x = self.fc3(self.x_tsne)

        return x


# 读取数据
data_dir = "./original"  # 设置自己路径
img_x, img_y = 256, 256  # 图片大小
vedio_path = []
labels = []

i = 0
for root, dirs, files in os.walk(data_dir):
    for file in files:
        path = os.path.join(root, file)
        label_pt = torch.tensor(i - 1)
        labels.append(label_pt)
        vedio_path.append(path)
    i += 1

#print(labels)

#print(vedio_path)

# train, test split
train_list, test_list, train_label, test_label = train_test_split(vedio_path, labels, test_size=0.1, random_state=2)

transform = transform = transforms.Compose([transforms.Resize([img_x, img_y]),
                                            transforms.ToTensor(),
                                            transforms.Normalize(mean=[0.5], std=[0.5])])

max_frames = 30


# 读取视频并抽取帧数
def get_center_image(frame):
    y, x = frame.shape[0:2]
    min_dim = min(y, x)
    start_x = (x // 2) - (min_dim // 2)
    start_y = (y // 2) - (min_dim // 2)

    return frame[start_y: start_y + min_dim, start_x: start_x + min_dim]


# 加载视频
def load_video(path, max_frames=0, use_transform=None, skip=20):
    # 跳帧读取，连续帧数效果一般
    cap = cv2.VideoCapture(path)
    frames = []
    i = 0
    try:
        while True:
            if i % skip == 0:
                ret, frame = cap.read()
                if not ret:
                    break
                frame = get_center_image(frame)
                frame = Image.fromarray(frame).convert('L')

                if use_transform is not None:
                    frame = use_transform(frame)

                frames.append(frame.squeeze_(0))

                if len(frames) == max_frames:
                    break
                i += 1
            else:
                i += 1
                continue

    finally:
        cap.release()
    return frames


# 3DCNN-Dataset
class Dataset_3DCNN(data.Dataset):
    def __init__(self, data_path, labels, max_frames, transform=None):
        """Initialization"""
        self.data_path = data_path
        self.labels = labels
        self.transform = transform
        self.frames = max_frames

    def __len__(self):
        """Denotes the total number of samples"""
        return len(self.data_path)

    def read_images(self, path, use_transform):
        X = load_video(path, self.frames, use_transform)
        X = torch.stack(X, dim=0)

        return X

    def __getitem__(self, index):
        """Generates one sample of data"""
        # 选择视频
        path = self.data_path[index]

        # 加载视频
        X = self.read_images(path, self.transform).unsqueeze_(0)  # (input) spatial images
        y = torch.LongTensor([self.labels[index]])  # (labels) LongTensor are for int64 instead of FloatTensor

        # print(X.shape)
        return X, y


train_set = Dataset_3DCNN(train_list, train_label, max_frames, transform=transform)
valid_set = Dataset_3DCNN(test_list, test_label, max_frames, transform=transform)

train_set.__getitem__(0)[0].shape

# 3D CNN 参数
fc_hidden1, fc_hidden2 = 256, 256
dropout = 0.0  # dropout probability

# 训练参数
k = 5  # 类别数
epochs = 50
batch_size = 25
learning_rate = 1e-4
log_interval = 1
img_x, img_y = 256, 256

save_model_path = "./3dcnnmodel"  # 设置保存模型路径

# Detect devices
use_cuda = torch.cuda.is_available()  # check if GPU exists
device = torch.device("cuda" if use_cuda else "cpu")  # use CPU or GPU

# load UCF101 actions names
params = {'batch_size': batch_size, 'shuffle': True, 'num_workers': 4, 'pin_memory': True} if use_cuda else {}

print(device)

train_loader = data.DataLoader(train_set, **params)
valid_loader = data.DataLoader(valid_set, **params)


def train(log_interval, model, device, train_loader, optimizer, epoch):
    # 训练模型
    model.train()

    losses = []
    scores = []
    N_count = 0  # 训练过的样本
    for batch_idx, (X, y) in enumerate(train_loader):

        X, y = X.to(device), y.to(device).view(-1, )
        N_count += X.size(0)

        optimizer.zero_grad()
        output = model(X)  # output size = (batch, number of classes)

        loss = F.cross_entropy(output, y)
        losses.append(loss.item())

        # 计算准确度
        y_pred = torch.max(output, 1)[1]  # y_pred != output

        accu_score = accuracy_score(y.cpu().data.squeeze().numpy(), y_pred.cpu().data.squeeze().numpy())
        balanced_score = balanced_accuracy_score(y.cpu().data.squeeze().numpy(), y_pred.cpu().data.squeeze().numpy())
        f1 = f1_score(y.cpu().data.squeeze().numpy(), y_pred.cpu().data.squeeze().numpy(), average="macro")
        weighted_f1 = f1_score(y.cpu().data.squeeze().numpy(), y_pred.cpu().data.squeeze().numpy(), average="weighted")
        step_score = [accu_score, balanced_score, f1, weighted_f1]
        scores.append(step_score)  # computed on CPU

        loss.backward()
        optimizer.step()

        # 显示信息
        if (batch_idx + 1) % log_interval == 0:
            print(
                'Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}, Accu: {:.2f}%, Weighted Accu: {:.2f}%, F1: {:.2f}%, Weighted F1: {:.2f}%'.format(
                    epoch + 1, N_count, len(train_loader.dataset), 100. * (batch_idx + 1) / len(train_loader),
                    loss.item(), 100 * step_score[0], 100 * step_score[1], 100 * step_score[2], 100 * step_score[3]))

    return losses, scores


def validation(model, device, optimizer, test_loader):
    # 验证模型
    model.eval()

    test_loss = 0
    all_y = []
    all_y_pred = []
    all_tsne = []
    with torch.no_grad():
        for X, y in test_loader:
            # distribute data to device
            X, y = X.to(device), y.to(device).view(-1, )

            output = model(X)
            output_tsne = model.x_tsne
            loss = F.cross_entropy(output, y, reduction='sum')
            test_loss += loss.item()  # sum up batch loss
            y_pred = output.max(1, keepdim=True)[1]  # (y_pred != output) get the index of the max log-probability

            # collect all y and y_pred in all batches
            all_y.extend(y)
            all_y_pred.extend(y_pred)
            all_tsne.extend(output_tsne)

    test_loss /= len(test_loader.dataset)

    # 计算准确度
    all_y = torch.stack(all_y, dim=0)
    all_y_pred = torch.stack(all_y_pred, dim=0)
    all_tsne = torch.stack(all_tsne, dim=0)

    accu_score = accuracy_score(all_y.cpu().data.squeeze().numpy(), all_y_pred.cpu().data.squeeze().numpy())
    balanced_score = balanced_accuracy_score(all_y.cpu().data.squeeze().numpy(),
                                             all_y_pred.cpu().data.squeeze().numpy())
    f1 = f1_score(all_y.cpu().data.squeeze().numpy(), all_y_pred.cpu().data.squeeze().numpy(), average="macro")
    weighted_f1 = f1_score(all_y.cpu().data.squeeze().numpy(), all_y_pred.cpu().data.squeeze().numpy(),
                           average="weighted")
    test_score = [accu_score, balanced_score, f1, weighted_f1]

    # 显示信息
    print(
        '\nTest set ({:d} samples): Average loss: {:.4f}, Accuracy: {:.2f}%, Weighted Accuracy: {:.2f}%, F1: {:.2f}%, Weighted F1: {:.2f}%\n'.format(
            len(all_y), test_loss, 100 * test_score[0], 100 * test_score[1], 100 * test_score[2], 100 * test_score[3]))
    datas = 'Average loss: ' + str(test_loss) + ' Accuracy: ' + str(100 * test_score[0]) + ' Weighted Accuracy: ' + str(
        100 * test_score[1]) + ' F1: ' + str(100 * test_score[2]) + ' Weighted F1: ' + str(100 * test_score[3])

    with open('3dcnndatas.txt', 'a', encoding='utf-8') as f:
        f.write(datas)
        f.write('\n')
        f.close()

    # 每10次迭代保存一次模型
    if epoch % 20 == 0:
        torch.save(model.state_dict(),
                   os.path.join(save_model_path, '3dcnn_epoch{}.pth'.format(epoch + 1)))  # save spatial_encoder
        torch.save(optimizer.state_dict(),
                   os.path.join(save_model_path, '3dcnn_optimizer_epoch{}.pth'.format(epoch + 1)))  # save optimizer
        print("Epoch {} model saved!".format(epoch + 1))

    return test_loss, test_score, all_y, all_y_pred, all_tsne


# 创建模型
cnn3d = CNN3D(t_dim=max_frames, img_x=img_x, img_y=img_y,
              drop_p=dropout, fc_hidden1=fc_hidden1, fc_hidden2=fc_hidden2, num_classes=k).to(device)

# 在多个GPU上平行运行
if torch.cuda.device_count() > 1:
    print("Using", torch.cuda.device_count(), "GPUs!")
    cnn3d = nn.DataParallel(cnn3d)

optimizer = torch.optim.Adam(cnn3d.parameters(), lr=learning_rate)  # 优化器

# 储存结果
epoch_train_losses = []
epoch_train_scores = []
epoch_test_losses = []
epoch_test_scores = []

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from sklearn.manifold import TSNE
import seaborn as sns
import itertools

def plt_tsne(gt_labels, pred_labels):
    tsne = TSNE(n_components=2, perplexity=5)
    X_tsne = tsne.fit_transform(pred_labels)
    X_tsne_data = np.vstack((X_tsne.T, gt_labels)).T
    df_tsne = pd.DataFrame(X_tsne_data, columns=['Dim1', 'Dim2', 'class'])
    plt.figure(figsize=(8, 8))
    sns.scatterplot(data=df_tsne, hue='class', x='Dim1', y='Dim2')
    plt.savefig('3dcnn_tsne.jpg')

def plot_confusion_matrix(gt_labels, pred_labels, normalize=True, title='Confusion matrix', cmap=plt.cm.Blues):
    """
    This function prints and plots the confusion matrix.
    Normalization can be applied by setting `normalize=True`.
    Input
    - cm : 计算出的混淆矩阵的值
    - classes : 混淆矩阵中每一行每一列对应的列
    - normalize : True:显示百分比, False:显示个数
    """
    cm = confusion_matrix(gt_labels, pred_labels)
    print("confusion_mat.shape : {}".format(cm.shape))
    print("confusion_mat : {}".format(cm))

    if normalize:
        matrix = cm
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        print("Normalized confusion matrix")
    else:
        print('Confusion matrix, without normalization')
    plt.figure()
    # 设置输出的图片大小
    figsize = 8, 8
    figure, ax = plt.subplots(figsize=figsize)
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    # 设置title的大小以及title的字体
    font_title = {'family': 'Times New Roman',
                  'weight': 'normal',
                  'size': 15,
                  }
    plt.title(title, fontdict=font_title)
    plt.colorbar()
    # tick_marks = np.arange(len(classes))
    # plt.xticks(tick_marks, classes, rotation=45, )
    # plt.yticks(tick_marks, classes)
    # 设置坐标刻度值的大小以及刻度值的字体
    plt.tick_params(labelsize=15)
    labels = ax.get_xticklabels() + ax.get_yticklabels()
    print(labels)
    [label.set_fontname('Times New Roman') for label in labels]
    if normalize:
        fm_int = 'd'
        fm_float = '.2%'
        thresh = cm.max() / 2.
        for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
            plt.text(j, i, format(cm[i, j], fm_float),
                     horizontalalignment="center", verticalalignment='bottom', family="Times New Roman",
                     weight="normal", size=15,
                     color="white" if cm[i, j] > thresh else "black")
            plt.text(j, i, format(matrix[i, j], fm_int),
                     horizontalalignment="center", verticalalignment='top', family="Times New Roman", weight="normal",
                     size=15,
                     color="white" if cm[i, j] > thresh else "black")
    else:
        fm_int = 'd'
        thresh = cm.max() / 2.
        for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
            plt.text(j, i, format(cm[i, j], fm_int),
                     horizontalalignment="center", verticalalignment='bottom',
                     color="white" if cm[i, j] > thresh else "black")

    plt.tight_layout()

    plt.savefig('3dcnn_cm.jpg')


# 训练
if __name__ == '__main__':
    for epoch in range(epochs):
        # train, test model
        train_losses, train_scores = train(log_interval, cnn3d, device, train_loader, optimizer, epoch)
        epoch_test_loss, epoch_test_score,all_y, all_y_pred, all_tsne = validation(cnn3d, device, optimizer, valid_loader)

        # 保存结果
        epoch_train_losses.append(train_losses)
        epoch_train_scores.append(train_scores)
        epoch_test_losses.append(epoch_test_loss)
        epoch_test_scores.append(epoch_test_score)

    # 保存全部结果
    A = np.array(epoch_train_losses)
    B = np.array(epoch_train_scores)
    C = np.array(epoch_test_losses)
    D = np.array(epoch_test_scores)
    np.save('./3DCNN_epoch_training_losses.npy', A)
    np.save('./3DCNN_epoch_training_scores.npy', B)
    np.save('./3DCNN_epoch_test_loss.npy', C)
    np.save('./3DCNN_epoch_test_score.npy', D)
    plot_confusion_matrix(all_y.cpu(), all_y_pred.cpu()[:, 0])
    plt_tsne(all_y.cpu(),all_tsne.cpu())
